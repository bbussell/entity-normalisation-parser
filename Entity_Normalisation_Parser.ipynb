{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity Normalisation Parser",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsQICz76z4Pe"
      },
      "source": [
        "Importing necessary packages, i.e. Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlYG62tfHgQK"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler"
      ],
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJzzj6RJwpCN"
      },
      "source": [
        "Loading the english nlp model and adding a named entity recognition (ner) pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vm10IVnSRlL"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner=nlp.get_pipe(\"ner\")"
      ],
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5K5xBYcwwQp"
      },
      "source": [
        "Ceating an EntityRuler pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ao-BDXMSWic"
      },
      "source": [
        "ruler = EntityRuler(nlp, overwrite_ents=True)"
      ],
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v5robEiw0yl"
      },
      "source": [
        "Loading in list of strings to be used for final testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe_44fGpH08Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb1b3b9-2ea4-4591-b57f-c40b23a1996b"
      },
      "source": [
        "og_strings = [\"MARKS AND SPENCER LTD\", \"LONDON\",\"ICNAO02312\",\n",
        "               \"LONDON, GREAT BRITAIN\", \"TOYS\", \"INTEL LLC\", \n",
        "               \"M&S CORPORATION Limited\", \"LONDON, ENGLAND\"]\n",
        "string_list = []\n",
        "for i in og_strings:\n",
        "  mod_str = i.lower()\n",
        "  string_list.append(mod_str)\n",
        "print(string_list)"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['marks and spencer ltd', 'london', 'icnao02312', 'london, great britain', 'toys', 'intel llc', 'm&s corporation limited', 'london, england']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Xw_nkcxukW"
      },
      "source": [
        "Viewing entity labels currently in the NER pipe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVKQeB_6V1Yn",
        "outputId": "3a1a76ae-edb7-41c1-e983-49647f67b055"
      },
      "source": [
        "labels = ner.labels\n",
        "print(labels)"
      ],
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOpF7o-gx1Et"
      },
      "source": [
        "If we test the NER before training, we'll see that it doesn't find our entity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiisQMtsShrB"
      },
      "source": [
        "doc = nlp(string_list[2]) "
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwlp0feox6Ri"
      },
      "source": [
        "Adding new entity label for address. Probably need lots more training examples for addresses for the model to be accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPv4pL25Yydb"
      },
      "source": [
        "LABELS = [\"ADDRESS\"]\n",
        "#ner.add_label(LABEL)\n",
        "for i in LABELS:\n",
        "  ner.add_label(i)"
      ],
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGSBvX9myAoj"
      },
      "source": [
        "Creating training data for the NER, which is in the format accepted by Spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikNxjni3Uc8Y"
      },
      "source": [
        "TRAIN_DATA =[\n",
        "             (\"An order from marks and spencer limited\", {\n",
        "                 'entities': [(14, 39, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for m&s\", {\n",
        "                 'entities': [(13, 16, 'ORG')] \n",
        "             }),\n",
        "             (\"Return for m&s limited\", {\n",
        "                 'entities': [(11, 22, 'ORG')] \n",
        "             }),\n",
        "             (\"Return for marks and spencer plc\", {\n",
        "                 'entities': [(11, 32, 'ORG')] \n",
        "             }),\n",
        "             (\"An order from nvidia\", {\n",
        "                 'entities': [(14, 20, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for nvidia ireland\", {\n",
        "                 'entities': [(13, 27, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for nvidia america\", {\n",
        "                 'entities': [(13, 27, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for nvidia corporation\", {\n",
        "                 'entities': [(13, 31, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for nvidia corp.\", {\n",
        "                 'entities': [(13, 25, 'ORG')] \n",
        "             }),\n",
        "             (\"An order from intel\", {\n",
        "                 'entities': [(14, 19, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for intel corporation\", {\n",
        "                 'entities': [(13, 30, 'ORG')] \n",
        "             }),\n",
        "              (\"apple\", {\n",
        "                 'entities': [(0, 5, 'ORG')]\n",
        "             }),\n",
        "             (\"An order from apple\", {\n",
        "                 'entities': [(14, 19, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for jl partnership\", {\n",
        "                 'entities': [(13, 27, 'ORG')] \n",
        "             }),\n",
        "             (\"An order from john lewis partnership\", {\n",
        "                 'entities': [(14, 36, 'ORG')] \n",
        "             }),\n",
        "             (\"Accounts for john lewis\", {\n",
        "                 'entities': [(13, 23, 'ORG')] \n",
        "             }),\n",
        "             (\"london, eng\", {\n",
        "                 'entities': [(0, 11, 'LOC')] \n",
        "             }),\n",
        "             (\"london\", {\n",
        "                 'entities': [(0, 6, 'LOC')] \n",
        "             }),\n",
        "             (\"london, uk\", {\n",
        "                 'entities': [(0, 10, 'LOC')] \n",
        "             }),\n",
        "             (\"asia\", {\n",
        "                 'entities': [(0, 4, 'LOC')]\n",
        "             }),\n",
        "              (\"east asia\", {\n",
        "                 'entities': [(0, 9, 'LOC')]\n",
        "             }),\n",
        "              (\"south asia\", {\n",
        "                 'entities': [(0, 10, 'LOC')]\n",
        "             }),\n",
        "             (\"beijing, china\", {\n",
        "                 'entities': [(0, 14, 'LOC')] \n",
        "             }),\n",
        "             (\"china\", {\n",
        "                 'entities': [(0, 5, 'LOC')] \n",
        "             }),\n",
        "             (\"shanghai, china\", {\n",
        "                 'entities': [(0, 15, 'LOC')] \n",
        "             }),\n",
        "             (\"ny, usa\", {\n",
        "                 'entities': [(0, 7, 'LOC')] \n",
        "             }),\n",
        "             (\"new york\", {\n",
        "                 'entities': [(0, 8, 'LOC')] \n",
        "             }),\n",
        "             (\"sydney, australia\", {\n",
        "                 'entities': [(0, 17, 'LOC')] \n",
        "             }),\n",
        "             (\"sydney, aus\", {\n",
        "                 'entities': [(0, 11, 'LOC')] \n",
        "             }),\n",
        "              (\"sydney\", {\n",
        "                 'entities': [(0, 6, 'LOC')] \n",
        "             }),\n",
        "             (\"Order for a hardwood table.\", {\n",
        "                 'entities': [(12, 26, 'PRODUCT')] \n",
        "             }),\n",
        "             (\"Invoice for a plastic chair\", {\n",
        "                 'entities': [(14, 27, 'PRODUCT')] \n",
        "             }),\n",
        "             (\"Invoice for a coffee table.\", {\n",
        "                 'entities': [(14, 26, 'PRODUCT')] \n",
        "             }),\n",
        "             (\"An invoice for a new plastic bottle\", {\n",
        "                 'entities': [(21, 35, 'PRODUCT')]\n",
        "             }),\n",
        "             (\"A fillet knife.\", {\n",
        "                 'entities': [(2, 14, 'PRODUCT')]\n",
        "             }),\n",
        "             (\"slough, se12 2xy\", {\n",
        "                 'entities': [(0, 16, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"33 timber yard, london, l1 8xy\", {\n",
        "                 'entities': [(0,30, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"44 china road, kowloon, hong kong\", {\n",
        "                 'entities': [(0, 33, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"100 north riverside plaza, chicago, illinois, united states\", {\n",
        "                 'entities': [(0, 58, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"1 apple park way, cupertino, california, united states\", {\n",
        "                 'entities': [(0, 54, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"santa clara, california, us\", {\n",
        "                 'entities': [(0, 27, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"2200 mission college blvd, santa clara, ca\", {\n",
        "                 'entities': [(0, 42, 'ADDRESS')]\n",
        "             }),\n",
        "             (\"1 hacker way, menlo park, ca\", {\n",
        "                 'entities': [(0, 28, 'ADDRESS')]\n",
        "             }),\n",
        "\n",
        "             \n",
        "]"
      ],
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESl-L_H9yJI1"
      },
      "source": [
        "Initialising nlp ready for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deMcFDN_U9Dy"
      },
      "source": [
        "optimizer = nlp.resume_training()"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdIHQRbyyNDQ"
      },
      "source": [
        "Excluding all other pipes during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dBrhVBZVAsp"
      },
      "source": [
        "# List of pipes you want to train\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "\n",
        "# List of pipes which should remain unaffected in training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_SYPJbFyTBV"
      },
      "source": [
        "Training the NER for 30 iterations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmpzXDEGVE9W",
        "outputId": "f2520cf3-a63e-4185-8e5f-11bf212f32fa"
      },
      "source": [
        "# Importing requirements\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "# Begin training by disabling other pipeline components\n",
        "with nlp.disable_pipes(*other_pipes) :\n",
        "\n",
        "  sizes = compounding(1.0, 4.0, 1.001)\n",
        "  # Training for 30 iterations     \n",
        "  for itn in range(30):\n",
        "    # shuffle examples before training\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "    # ictionary to store losses\n",
        "    losses = {}\n",
        "    for batch in batches:\n",
        "      texts, annotations = zip(*batch)\n",
        "      # Calling update() over the iteration\n",
        "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "      print(\"Losses\", losses)"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses {'ner': 6.380213725464554}\n",
            "Losses {'ner': 10.652604329483719}\n",
            "Losses {'ner': 16.30599134494704}\n",
            "Losses {'ner': 18.239293983795584}\n",
            "Losses {'ner': 18.24222218746589}\n",
            "Losses {'ner': 20.136048552936778}\n",
            "Losses {'ner': 24.878168222851023}\n",
            "Losses {'ner': 31.853354252582072}\n",
            "Losses {'ner': 33.245430029635905}\n",
            "Losses {'ner': 43.893124855762004}\n",
            "Losses {'ner': 46.006511397367}\n",
            "Losses {'ner': 51.23824853209779}\n",
            "Losses {'ner': 53.23645443229005}\n",
            "Losses {'ner': 55.20582074431703}\n",
            "Losses {'ner': 57.1929019025058}\n",
            "Losses {'ner': 61.81701086906309}\n",
            "Losses {'ner': 61.81877038281004}\n",
            "Losses {'ner': 68.25581439183208}\n",
            "Losses {'ner': 69.09829975120395}\n",
            "Losses {'ner': 70.66871465595499}\n",
            "Losses {'ner': 74.92006589325204}\n",
            "Losses {'ner': 76.93398508067797}\n",
            "Losses {'ner': 82.84156491990755}\n",
            "Losses {'ner': 90.07273339525885}\n",
            "Losses {'ner': 100.84488522400237}\n",
            "Losses {'ner': 104.66733031538637}\n",
            "Losses {'ner': 107.25011365679414}\n",
            "Losses {'ner': 110.7575196537425}\n",
            "Losses {'ner': 114.10732429355879}\n",
            "Losses {'ner': 119.24892886609335}\n",
            "Losses {'ner': 122.10929976564397}\n",
            "Losses {'ner': 124.1845894113237}\n",
            "Losses {'ner': 127.4800325885469}\n",
            "Losses {'ner': 131.7476986900026}\n",
            "Losses {'ner': 131.98980061077123}\n",
            "Losses {'ner': 135.75858472062754}\n",
            "Losses {'ner': 140.4296379520885}\n",
            "Losses {'ner': 140.4296379520885}\n",
            "Losses {'ner': 151.80430302683743}\n",
            "Losses {'ner': 153.78827022664262}\n",
            "Losses {'ner': 161.23668001969708}\n",
            "Losses {'ner': 163.45720025156987}\n",
            "Losses {'ner': 168.67607291792882}\n",
            "Losses {'ner': 0.041438327202740766}\n",
            "Losses {'ner': 2.038397461207751}\n",
            "Losses {'ner': 3.1691482912601714}\n",
            "Losses {'ner': 6.055992376091126}\n",
            "Losses {'ner': 9.636296555431727}\n",
            "Losses {'ner': 12.629591589126903}\n",
            "Losses {'ner': 12.629591589126903}\n",
            "Losses {'ner': 17.206715144918235}\n",
            "Losses {'ner': 20.109838093284452}\n",
            "Losses {'ner': 24.784249436695063}\n",
            "Losses {'ner': 35.11643777473637}\n",
            "Losses {'ner': 37.949985754329646}\n",
            "Losses {'ner': 40.337207630861485}\n",
            "Losses {'ner': 40.66670613049337}\n",
            "Losses {'ner': 40.67008138382755}\n",
            "Losses {'ner': 50.00972218001209}\n",
            "Losses {'ner': 53.07696340167843}\n",
            "Losses {'ner': 62.00290391528927}\n",
            "Losses {'ner': 63.266202069166525}\n",
            "Losses {'ner': 65.79567048633419}\n",
            "Losses {'ner': 70.06747225606762}\n",
            "Losses {'ner': 72.92170907497587}\n",
            "Losses {'ner': 74.23589490737015}\n",
            "Losses {'ner': 77.09709047164017}\n",
            "Losses {'ner': 78.35851078863436}\n",
            "Losses {'ner': 79.71177673257137}\n",
            "Losses {'ner': 80.97930353286085}\n",
            "Losses {'ner': 82.68315651465548}\n",
            "Losses {'ner': 87.32229337666524}\n",
            "Losses {'ner': 88.44372663472188}\n",
            "Losses {'ner': 92.1858522651006}\n",
            "Losses {'ner': 102.02879383007405}\n",
            "Losses {'ner': 108.83510799205271}\n",
            "Losses {'ner': 110.22077829635111}\n",
            "Losses {'ner': 111.72419197356669}\n",
            "Losses {'ner': 113.69292965973602}\n",
            "Losses {'ner': 120.93048251117455}\n",
            "Losses {'ner': 124.0306741533948}\n",
            "Losses {'ner': 128.68155153693237}\n",
            "Losses {'ner': 134.81300060303488}\n",
            "Losses {'ner': 139.8427856400756}\n",
            "Losses {'ner': 140.83016792329198}\n",
            "Losses {'ner': 144.00080304237252}\n",
            "Losses {'ner': 2.6175346529780654}\n",
            "Losses {'ner': 5.54142856974795}\n",
            "Losses {'ner': 7.528136741413618}\n",
            "Losses {'ner': 7.689969359672432}\n",
            "Losses {'ner': 11.27757736924923}\n",
            "Losses {'ner': 17.577996476328735}\n",
            "Losses {'ner': 21.67103275378122}\n",
            "Losses {'ner': 25.584804982886453}\n",
            "Losses {'ner': 32.12046593521609}\n",
            "Losses {'ner': 36.084204370164116}\n",
            "Losses {'ner': 36.089388656349456}\n",
            "Losses {'ner': 37.24668018721377}\n",
            "Losses {'ner': 39.99103306621646}\n",
            "Losses {'ner': 42.10864058757534}\n",
            "Losses {'ner': 43.66508300175292}\n",
            "Losses {'ner': 51.3401799904309}\n",
            "Losses {'ner': 57.518962032546824}\n",
            "Losses {'ner': 58.83629831479141}\n",
            "Losses {'ner': 62.51998163939663}\n",
            "Losses {'ner': 71.49744208523282}\n",
            "Losses {'ner': 73.30663254105696}\n",
            "Losses {'ner': 77.94608358517382}\n",
            "Losses {'ner': 82.85225985422824}\n",
            "Losses {'ner': 87.29386443123315}\n",
            "Losses {'ner': 90.58771813139413}\n",
            "Losses {'ner': 91.56080329641793}\n",
            "Losses {'ner': 93.11849144324754}\n",
            "Losses {'ner': 93.11849144324754}\n",
            "Losses {'ner': 94.13357880935166}\n",
            "Losses {'ner': 96.95190867260192}\n",
            "Losses {'ner': 98.05599089935515}\n",
            "Losses {'ner': 102.80552426949237}\n",
            "Losses {'ner': 105.0152427595118}\n",
            "Losses {'ner': 108.60120552000683}\n",
            "Losses {'ner': 112.15127506819647}\n",
            "Losses {'ner': 112.16583231990319}\n",
            "Losses {'ner': 115.98185411994439}\n",
            "Losses {'ner': 123.27819529143255}\n",
            "Losses {'ner': 123.5197759565441}\n",
            "Losses {'ner': 123.55196255149633}\n",
            "Losses {'ner': 125.80186490612431}\n",
            "Losses {'ner': 134.82431599395676}\n",
            "Losses {'ner': 137.7513397027771}\n",
            "Losses {'ner': 1.7243874026462436e-05}\n",
            "Losses {'ner': 1.8442072975158226}\n",
            "Losses {'ner': 4.69783980117063}\n",
            "Losses {'ner': 8.987868921336485}\n",
            "Losses {'ner': 9.748979048308684}\n",
            "Losses {'ner': 14.224196629336802}\n",
            "Losses {'ner': 14.227524993679253}\n",
            "Losses {'ner': 15.022151050827233}\n",
            "Losses {'ner': 17.96902452383074}\n",
            "Losses {'ner': 19.99233501168783}\n",
            "Losses {'ner': 22.65882258626516}\n",
            "Losses {'ner': 26.596992751088692}\n",
            "Losses {'ner': 30.58662249657209}\n",
            "Losses {'ner': 30.61275317800755}\n",
            "Losses {'ner': 30.61275317800755}\n",
            "Losses {'ner': 34.13049906028027}\n",
            "Losses {'ner': 37.39107109887118}\n",
            "Losses {'ner': 38.52358760133326}\n",
            "Losses {'ner': 39.778702991157175}\n",
            "Losses {'ner': 45.26922275319636}\n",
            "Losses {'ner': 49.15975578322947}\n",
            "Losses {'ner': 57.51052927419484}\n",
            "Losses {'ner': 57.51053134394466}\n",
            "Losses {'ner': 62.47983298493206}\n",
            "Losses {'ner': 63.542414047714615}\n",
            "Losses {'ner': 63.54275795673629}\n",
            "Losses {'ner': 65.60605973606607}\n",
            "Losses {'ner': 68.07146878024241}\n",
            "Losses {'ner': 70.29858650452744}\n",
            "Losses {'ner': 75.94688694364677}\n",
            "Losses {'ner': 78.55047805792938}\n",
            "Losses {'ner': 85.14987689069758}\n",
            "Losses {'ner': 89.9945241294271}\n",
            "Losses {'ner': 93.67497734664076}\n",
            "Losses {'ner': 100.45994080541723}\n",
            "Losses {'ner': 106.88629530308836}\n",
            "Losses {'ner': 110.33661482794278}\n",
            "Losses {'ner': 111.99170864684575}\n",
            "Losses {'ner': 115.37521758000085}\n",
            "Losses {'ner': 120.07909564790035}\n",
            "Losses {'ner': 121.90373014174588}\n",
            "Losses {'ner': 126.03864761085902}\n",
            "Losses {'ner': 130.3880085620815}\n",
            "Losses {'ner': 1.2632333040237427}\n",
            "Losses {'ner': 3.513933977112174}\n",
            "Losses {'ner': 10.578447366133332}\n",
            "Losses {'ner': 11.288897783029824}\n",
            "Losses {'ner': 12.63087010430172}\n",
            "Losses {'ner': 16.287674427498132}\n",
            "Losses {'ner': 17.72265398548916}\n",
            "Losses {'ner': 26.237589319702238}\n",
            "Losses {'ner': 29.814095221067415}\n",
            "Losses {'ner': 30.724187714586602}\n",
            "Losses {'ner': 34.53326609086116}\n",
            "Losses {'ner': 37.8438296551617}\n",
            "Losses {'ner': 39.818110098204215}\n",
            "Losses {'ner': 44.83124615725001}\n",
            "Losses {'ner': 48.57642886873555}\n",
            "Losses {'ner': 51.70287478847649}\n",
            "Losses {'ner': 52.80409325046685}\n",
            "Losses {'ner': 58.05486411733773}\n",
            "Losses {'ner': 65.46265576167252}\n",
            "Losses {'ner': 67.8996662531431}\n",
            "Losses {'ner': 69.86825485587224}\n",
            "Losses {'ner': 74.07795210242375}\n",
            "Losses {'ner': 78.18912120223149}\n",
            "Losses {'ner': 85.63642655372723}\n",
            "Losses {'ner': 87.59090089108076}\n",
            "Losses {'ner': 87.62111290413168}\n",
            "Losses {'ner': 88.80649308170405}\n",
            "Losses {'ner': 98.04665341283408}\n",
            "Losses {'ner': 100.14294700705659}\n",
            "Losses {'ner': 104.87007195138585}\n",
            "Losses {'ner': 107.46658706673634}\n",
            "Losses {'ner': 111.56172859677326}\n",
            "Losses {'ner': 118.95508944996845}\n",
            "Losses {'ner': 118.95508944996845}\n",
            "Losses {'ner': 125.61189442509067}\n",
            "Losses {'ner': 131.1020081606971}\n",
            "Losses {'ner': 132.08543431435692}\n",
            "Losses {'ner': 133.8488690171912}\n",
            "Losses {'ner': 135.1841805827096}\n",
            "Losses {'ner': 137.70064852626433}\n",
            "Losses {'ner': 139.7671820461877}\n",
            "Losses {'ner': 142.24029395166542}\n",
            "Losses {'ner': 143.31883606496956}\n",
            "Losses {'ner': 6.547095246613026}\n",
            "Losses {'ner': 8.819986835987947}\n",
            "Losses {'ner': 8.99468895879545}\n",
            "Losses {'ner': 9.000090793251957}\n",
            "Losses {'ner': 9.000090793251957}\n",
            "Losses {'ner': 9.000090978262207}\n",
            "Losses {'ner': 9.520300026402012}\n",
            "Losses {'ner': 10.812869324699179}\n",
            "Losses {'ner': 13.426886706701382}\n",
            "Losses {'ner': 23.252624127170904}\n",
            "Losses {'ner': 26.045622977516516}\n",
            "Losses {'ner': 26.42390418789107}\n",
            "Losses {'ner': 28.55619657776076}\n",
            "Losses {'ner': 30.8578390627237}\n",
            "Losses {'ner': 34.81713397792298}\n",
            "Losses {'ner': 37.32483943170625}\n",
            "Losses {'ner': 42.19736989206392}\n",
            "Losses {'ner': 47.169069737575384}\n",
            "Losses {'ner': 49.49780398466207}\n",
            "Losses {'ner': 50.53296094991781}\n",
            "Losses {'ner': 57.15228348829366}\n",
            "Losses {'ner': 63.05273777105428}\n",
            "Losses {'ner': 65.31363284446813}\n",
            "Losses {'ner': 66.12879443027593}\n",
            "Losses {'ner': 69.26121263810016}\n",
            "Losses {'ner': 70.36091932733432}\n",
            "Losses {'ner': 74.63165750247315}\n",
            "Losses {'ner': 79.68655135375336}\n",
            "Losses {'ner': 81.46495477241585}\n",
            "Losses {'ner': 84.58274652940342}\n",
            "Losses {'ner': 91.28924938422749}\n",
            "Losses {'ner': 95.21217544508288}\n",
            "Losses {'ner': 97.16053683009993}\n",
            "Losses {'ner': 99.97444347680782}\n",
            "Losses {'ner': 101.28874152542328}\n",
            "Losses {'ner': 104.38595962074821}\n",
            "Losses {'ner': 105.35932462957923}\n",
            "Losses {'ner': 107.36407398727958}\n",
            "Losses {'ner': 110.47707652596061}\n",
            "Losses {'ner': 115.85468317165275}\n",
            "Losses {'ner': 120.60195470943351}\n",
            "Losses {'ner': 123.21114766899034}\n",
            "Losses {'ner': 126.53521532121583}\n",
            "Losses {'ner': 5.367005720734596}\n",
            "Losses {'ner': 9.44839869812131}\n",
            "Losses {'ner': 10.804759255493991}\n",
            "Losses {'ner': 13.06287877901923}\n",
            "Losses {'ner': 15.775235659326427}\n",
            "Losses {'ner': 22.181674486841075}\n",
            "Losses {'ner': 29.231814282829873}\n",
            "Losses {'ner': 31.7252007752395}\n",
            "Losses {'ner': 33.09444222149614}\n",
            "Losses {'ner': 33.727636877360055}\n",
            "Losses {'ner': 37.63216954833479}\n",
            "Losses {'ner': 38.678679194214055}\n",
            "Losses {'ner': 45.19809040045948}\n",
            "Losses {'ner': 45.95131645906076}\n",
            "Losses {'ner': 53.05147268694418}\n",
            "Losses {'ner': 54.6581710807659}\n",
            "Losses {'ner': 60.054436647114926}\n",
            "Losses {'ner': 61.43495374744816}\n",
            "Losses {'ner': 63.38645195787831}\n",
            "Losses {'ner': 65.42931248607783}\n",
            "Losses {'ner': 65.43027310107573}\n",
            "Losses {'ner': 67.43063987214555}\n",
            "Losses {'ner': 71.62625479679991}\n",
            "Losses {'ner': 73.68373559304018}\n",
            "Losses {'ner': 77.03789688954566}\n",
            "Losses {'ner': 81.78123909303395}\n",
            "Losses {'ner': 86.26425727255264}\n",
            "Losses {'ner': 92.12373935989308}\n",
            "Losses {'ner': 94.76997764014959}\n",
            "Losses {'ner': 97.09983114853821}\n",
            "Losses {'ner': 98.26458247585106}\n",
            "Losses {'ner': 98.27282793374616}\n",
            "Losses {'ner': 103.84898737931508}\n",
            "Losses {'ner': 103.84928800899047}\n",
            "Losses {'ner': 103.84928800899047}\n",
            "Losses {'ner': 105.86437811853807}\n",
            "Losses {'ner': 106.86052652599733}\n",
            "Losses {'ner': 108.12511668831553}\n",
            "Losses {'ner': 114.43528506264533}\n",
            "Losses {'ner': 117.28991182928416}\n",
            "Losses {'ner': 117.44824668054912}\n",
            "Losses {'ner': 119.84029581432674}\n",
            "Losses {'ner': 128.0223667476091}\n",
            "Losses {'ner': 2.157150445935258}\n",
            "Losses {'ner': 8.954792855736741}\n",
            "Losses {'ner': 12.387685059067735}\n",
            "Losses {'ner': 13.976734147894604}\n",
            "Losses {'ner': 17.77227519594453}\n",
            "Losses {'ner': 17.77227519594453}\n",
            "Losses {'ner': 19.351291097940702}\n",
            "Losses {'ner': 25.062371813000937}\n",
            "Losses {'ner': 27.554242791033175}\n",
            "Losses {'ner': 31.846835428061418}\n",
            "Losses {'ner': 32.3623253285461}\n",
            "Losses {'ner': 33.47871797114539}\n",
            "Losses {'ner': 35.22052149127166}\n",
            "Losses {'ner': 38.49998705300431}\n",
            "Losses {'ner': 40.1639948835092}\n",
            "Losses {'ner': 42.52592596909582}\n",
            "Losses {'ner': 49.67601839205801}\n",
            "Losses {'ner': 49.679814922196435}\n",
            "Losses {'ner': 51.19316382942872}\n",
            "Losses {'ner': 54.16245021139422}\n",
            "Losses {'ner': 61.59833254133502}\n",
            "Losses {'ner': 64.01911381798948}\n",
            "Losses {'ner': 64.09353317665224}\n",
            "Losses {'ner': 67.16976097291624}\n",
            "Losses {'ner': 69.30626716437106}\n",
            "Losses {'ner': 69.33986087116074}\n",
            "Losses {'ner': 69.33995021972379}\n",
            "Losses {'ner': 78.65425521228036}\n",
            "Losses {'ner': 83.11559601220807}\n",
            "Losses {'ner': 90.62240703496656}\n",
            "Losses {'ner': 96.2523657092782}\n",
            "Losses {'ner': 99.20407039019784}\n",
            "Losses {'ner': 100.73475232165521}\n",
            "Losses {'ner': 104.37144038830166}\n",
            "Losses {'ner': 108.30983524959748}\n",
            "Losses {'ner': 108.30994179100827}\n",
            "Losses {'ner': 109.69262281550192}\n",
            "Losses {'ner': 113.2169492591741}\n",
            "Losses {'ner': 113.21695555612024}\n",
            "Losses {'ner': 122.00869651004251}\n",
            "Losses {'ner': 122.98552383347925}\n",
            "Losses {'ner': 122.98604260017675}\n",
            "Losses {'ner': 125.35401263293198}\n",
            "Losses {'ner': 0.0025516508420935224}\n",
            "Losses {'ner': 0.002551657829850118}\n",
            "Losses {'ner': 2.2206312247854165}\n",
            "Losses {'ner': 4.354480324063866}\n",
            "Losses {'ner': 10.7060534858447}\n",
            "Losses {'ner': 11.821716895077794}\n",
            "Losses {'ner': 15.574031023059456}\n",
            "Losses {'ner': 17.559602673913126}\n",
            "Losses {'ner': 17.559602673913126}\n",
            "Losses {'ner': 18.62064383898142}\n",
            "Losses {'ner': 19.849379257604408}\n",
            "Losses {'ner': 24.055915083575922}\n",
            "Losses {'ner': 25.961689857981337}\n",
            "Losses {'ner': 28.410267386335118}\n",
            "Losses {'ner': 32.46697286118864}\n",
            "Losses {'ner': 33.43806067456602}\n",
            "Losses {'ner': 34.37428613200792}\n",
            "Losses {'ner': 38.314379372232004}\n",
            "Losses {'ner': 38.77401874157568}\n",
            "Losses {'ner': 41.50248519856451}\n",
            "Losses {'ner': 43.61040148401367}\n",
            "Losses {'ner': 44.504169877063994}\n",
            "Losses {'ner': 47.8368562119124}\n",
            "Losses {'ner': 49.69225285519338}\n",
            "Losses {'ner': 55.9825114956168}\n",
            "Losses {'ner': 57.03582058352027}\n",
            "Losses {'ner': 59.697846501142614}\n",
            "Losses {'ner': 65.38290783670627}\n",
            "Losses {'ner': 73.5276787029662}\n",
            "Losses {'ner': 75.8191115600649}\n",
            "Losses {'ner': 83.34320536680852}\n",
            "Losses {'ner': 89.95341214300524}\n",
            "Losses {'ner': 92.14162163339014}\n",
            "Losses {'ner': 95.11915128544652}\n",
            "Losses {'ner': 96.08473192766988}\n",
            "Losses {'ner': 96.0930161006444}\n",
            "Losses {'ner': 97.86470489556532}\n",
            "Losses {'ner': 99.62168780000584}\n",
            "Losses {'ner': 103.22090914876836}\n",
            "Losses {'ner': 108.78479334743398}\n",
            "Losses {'ner': 110.28624157732057}\n",
            "Losses {'ner': 113.3823811753654}\n",
            "Losses {'ner': 118.75006739487881}\n",
            "Losses {'ner': 3.721365717923618}\n",
            "Losses {'ner': 4.732271120286896}\n",
            "Losses {'ner': 5.760461493922776}\n",
            "Losses {'ner': 5.760461506524372}\n",
            "Losses {'ner': 10.645163414159107}\n",
            "Losses {'ner': 13.344809217003423}\n",
            "Losses {'ner': 20.773256344345647}\n",
            "Losses {'ner': 24.82796443542211}\n",
            "Losses {'ner': 28.066570292317557}\n",
            "Losses {'ner': 30.271311861553478}\n",
            "Losses {'ner': 33.4365200922799}\n",
            "Losses {'ner': 34.56814746660857}\n",
            "Losses {'ner': 39.15290308147414}\n",
            "Losses {'ner': 42.956687690315086}\n",
            "Losses {'ner': 46.5110401526068}\n",
            "Losses {'ner': 46.51104362652732}\n",
            "Losses {'ner': 47.4989627552119}\n",
            "Losses {'ner': 47.529772015636475}\n",
            "Losses {'ner': 47.715387526949264}\n",
            "Losses {'ner': 50.59876327291443}\n",
            "Losses {'ner': 54.93242912069275}\n",
            "Losses {'ner': 60.94741024524286}\n",
            "Losses {'ner': 60.965544971454285}\n",
            "Losses {'ner': 61.93476274870648}\n",
            "Losses {'ner': 62.98062006181329}\n",
            "Losses {'ner': 69.20260728692858}\n",
            "Losses {'ner': 79.04598359316344}\n",
            "Losses {'ner': 83.30073754414673}\n",
            "Losses {'ner': 84.8844861604249}\n",
            "Losses {'ner': 88.31317677274103}\n",
            "Losses {'ner': 96.84835641727307}\n",
            "Losses {'ner': 99.30768607464609}\n",
            "Losses {'ner': 103.44145007339296}\n",
            "Losses {'ner': 107.62058373774391}\n",
            "Losses {'ner': 111.56431350811374}\n",
            "Losses {'ner': 113.51867568540376}\n",
            "Losses {'ner': 116.3996386495511}\n",
            "Losses {'ner': 118.48793410929841}\n",
            "Losses {'ner': 118.48793410929841}\n",
            "Losses {'ner': 121.49939513707989}\n",
            "Losses {'ner': 124.5981476357066}\n",
            "Losses {'ner': 127.72516125942104}\n",
            "Losses {'ner': 127.72535653375428}\n",
            "Losses {'ner': 1.349690318745104}\n",
            "Losses {'ner': 5.991569787901369}\n",
            "Losses {'ner': 9.28647431794974}\n",
            "Losses {'ner': 10.766648486894098}\n",
            "Losses {'ner': 11.732902959626642}\n",
            "Losses {'ner': 13.238764446569746}\n",
            "Losses {'ner': 13.238818712618865}\n",
            "Losses {'ner': 14.20079375067047}\n",
            "Losses {'ner': 16.41689537474207}\n",
            "Losses {'ner': 21.126513087917317}\n",
            "Losses {'ner': 24.891744269893877}\n",
            "Losses {'ner': 33.82254193620514}\n",
            "Losses {'ner': 36.919863874804776}\n",
            "Losses {'ner': 36.919863874804776}\n",
            "Losses {'ner': 38.62619045802837}\n",
            "Losses {'ner': 42.155769967165334}\n",
            "Losses {'ner': 47.09044411095653}\n",
            "Losses {'ner': 52.93039356979404}\n",
            "Losses {'ner': 55.89708595785175}\n",
            "Losses {'ner': 58.65742176088367}\n",
            "Losses {'ner': 65.18833045164916}\n",
            "Losses {'ner': 66.10266957680557}\n",
            "Losses {'ner': 68.51617327022228}\n",
            "Losses {'ner': 72.4974648623434}\n",
            "Losses {'ner': 77.30351933526669}\n",
            "Losses {'ner': 78.26657125043545}\n",
            "Losses {'ner': 79.63886897991739}\n",
            "Losses {'ner': 81.86597334574304}\n",
            "Losses {'ner': 83.84998237322412}\n",
            "Losses {'ner': 85.37516624024698}\n",
            "Losses {'ner': 87.81843278101275}\n",
            "Losses {'ner': 92.38910075770087}\n",
            "Losses {'ner': 93.83281703809604}\n",
            "Losses {'ner': 96.78001302566241}\n",
            "Losses {'ner': 101.26037897255134}\n",
            "Losses {'ner': 106.10033477928351}\n",
            "Losses {'ner': 107.11148423121247}\n",
            "Losses {'ner': 115.58080730006967}\n",
            "Losses {'ner': 118.30067828462396}\n",
            "Losses {'ner': 119.46583346263995}\n",
            "Losses {'ner': 124.78330454961886}\n",
            "Losses {'ner': 126.86981699229707}\n",
            "Losses {'ner': 129.08355730253768}\n",
            "Losses {'ner': 0.0008964480948634446}\n",
            "Losses {'ner': 4.138390434003668}\n",
            "Losses {'ner': 7.52519452109118}\n",
            "Losses {'ner': 9.390566925532767}\n",
            "Losses {'ner': 9.87097123893463}\n",
            "Losses {'ner': 11.097809052788847}\n",
            "Losses {'ner': 16.240585465590897}\n",
            "Losses {'ner': 16.25913949930245}\n",
            "Losses {'ner': 22.91480149948174}\n",
            "Losses {'ner': 25.725895273931286}\n",
            "Losses {'ner': 26.694987285383007}\n",
            "Losses {'ner': 29.645218306762303}\n",
            "Losses {'ner': 32.453517306671614}\n",
            "Losses {'ner': 35.548595596180434}\n",
            "Losses {'ner': 35.83567007277733}\n",
            "Losses {'ner': 35.83567007277733}\n",
            "Losses {'ner': 36.9652796947058}\n",
            "Losses {'ner': 42.40576958289702}\n",
            "Losses {'ner': 49.061925884395436}\n",
            "Losses {'ner': 49.225440241555674}\n",
            "Losses {'ner': 52.90120742485951}\n",
            "Losses {'ner': 56.2238194124503}\n",
            "Losses {'ner': 58.1422210585681}\n",
            "Losses {'ner': 59.10786506052885}\n",
            "Losses {'ner': 59.14855682520245}\n",
            "Losses {'ner': 68.72857706943576}\n",
            "Losses {'ner': 69.68011316606824}\n",
            "Losses {'ner': 71.71398338252364}\n",
            "Losses {'ner': 74.97957911664305}\n",
            "Losses {'ner': 75.09118897131607}\n",
            "Losses {'ner': 80.40629959515259}\n",
            "Losses {'ner': 85.86344141083732}\n",
            "Losses {'ner': 89.7032156320192}\n",
            "Losses {'ner': 97.04403665977493}\n",
            "Losses {'ner': 104.95449643152729}\n",
            "Losses {'ner': 105.92923491018787}\n",
            "Losses {'ner': 110.25441371220127}\n",
            "Losses {'ner': 114.0017637492992}\n",
            "Losses {'ner': 115.33751803641549}\n",
            "Losses {'ner': 119.00971752410165}\n",
            "Losses {'ner': 121.38186086716355}\n",
            "Losses {'ner': 121.84283774883711}\n",
            "Losses {'ner': 124.24995386393034}\n",
            "Losses {'ner': 0.025627754128734637}\n",
            "Losses {'ner': 0.03681219514824363}\n",
            "Losses {'ner': 2.2591892569921157}\n",
            "Losses {'ner': 8.654131749238744}\n",
            "Losses {'ner': 9.71214425052058}\n",
            "Losses {'ner': 10.656442815908306}\n",
            "Losses {'ner': 14.62955980981242}\n",
            "Losses {'ner': 17.69891260666664}\n",
            "Losses {'ner': 19.565693749606183}\n",
            "Losses {'ner': 23.6987787523869}\n",
            "Losses {'ner': 26.336934448859566}\n",
            "Losses {'ner': 28.827826471707695}\n",
            "Losses {'ner': 33.17546591319787}\n",
            "Losses {'ner': 39.51957017457552}\n",
            "Losses {'ner': 40.75740114132641}\n",
            "Losses {'ner': 45.95866588036297}\n",
            "Losses {'ner': 51.98255715797423}\n",
            "Losses {'ner': 55.65812925527571}\n",
            "Losses {'ner': 56.67674504243662}\n",
            "Losses {'ner': 62.327409193151496}\n",
            "Losses {'ner': 69.98848567926218}\n",
            "Losses {'ner': 74.42741201841166}\n",
            "Losses {'ner': 74.42741201841166}\n",
            "Losses {'ner': 75.42035584174921}\n",
            "Losses {'ner': 75.43142507456726}\n",
            "Losses {'ner': 76.5961349378202}\n",
            "Losses {'ner': 84.54563824039435}\n",
            "Losses {'ner': 85.56885159319957}\n",
            "Losses {'ner': 85.57006800187976}\n",
            "Losses {'ner': 87.69854991896302}\n",
            "Losses {'ner': 91.69418910563202}\n",
            "Losses {'ner': 99.3433107868603}\n",
            "Losses {'ner': 102.69433983276924}\n",
            "Losses {'ner': 103.9051364762225}\n",
            "Losses {'ner': 105.73294873575544}\n",
            "Losses {'ner': 107.01910551133065}\n",
            "Losses {'ner': 108.3411276961439}\n",
            "Losses {'ner': 113.51838996158281}\n",
            "Losses {'ner': 113.54301559449473}\n",
            "Losses {'ner': 118.93024008990565}\n",
            "Losses {'ner': 120.35308878635804}\n",
            "Losses {'ner': 121.31371276592652}\n",
            "Losses {'ner': 127.60855936026017}\n",
            "Losses {'ner': 5.053994178771973}\n",
            "Losses {'ner': 5.053994178771973}\n",
            "Losses {'ner': 7.043718740757555}\n",
            "Losses {'ner': 7.260878634092165}\n",
            "Losses {'ner': 7.260878634295926}\n",
            "Losses {'ner': 12.730495567061274}\n",
            "Losses {'ner': 18.560819345154137}\n",
            "Losses {'ner': 23.691184701957077}\n",
            "Losses {'ner': 26.675492383400247}\n",
            "Losses {'ner': 29.2660709170188}\n",
            "Losses {'ner': 33.23512328287286}\n",
            "Losses {'ner': 34.74177948342482}\n",
            "Losses {'ner': 34.815728055576315}\n",
            "Losses {'ner': 35.962841075079524}\n",
            "Losses {'ner': 40.6744422727991}\n",
            "Losses {'ner': 41.11389320342849}\n",
            "Losses {'ner': 41.12384013675026}\n",
            "Losses {'ner': 45.11310033646285}\n",
            "Losses {'ner': 49.255927953645624}\n",
            "Losses {'ner': 54.88141603601824}\n",
            "Losses {'ner': 59.73408515585314}\n",
            "Losses {'ner': 62.517316831477984}\n",
            "Losses {'ner': 62.51847128433863}\n",
            "Losses {'ner': 69.05187745889478}\n",
            "Losses {'ner': 80.3642359390308}\n",
            "Losses {'ner': 85.75406450176732}\n",
            "Losses {'ner': 85.75558245408072}\n",
            "Losses {'ner': 88.24988072514638}\n",
            "Losses {'ner': 89.22275006175146}\n",
            "Losses {'ner': 91.42062477663988}\n",
            "Losses {'ner': 95.16627492436076}\n",
            "Losses {'ner': 101.22965224512721}\n",
            "Losses {'ner': 101.22965235829349}\n",
            "Losses {'ner': 106.22933979153629}\n",
            "Losses {'ner': 107.19121421694751}\n",
            "Losses {'ner': 110.16496049806472}\n",
            "Losses {'ner': 115.8662711433152}\n",
            "Losses {'ner': 116.6740397126431}\n",
            "Losses {'ner': 116.67403971300743}\n",
            "Losses {'ner': 118.96292169344312}\n",
            "Losses {'ner': 124.92169806313402}\n",
            "Losses {'ner': 127.22192998054604}\n",
            "Losses {'ner': 129.1983933338947}\n",
            "Losses {'ner': 6.115378379821777}\n",
            "Losses {'ner': 8.417338301282143}\n",
            "Losses {'ner': 11.894601036648965}\n",
            "Losses {'ner': 11.894601759781324}\n",
            "Losses {'ner': 14.425663713355277}\n",
            "Losses {'ner': 17.120104708087126}\n",
            "Losses {'ner': 17.131564574466477}\n",
            "Losses {'ner': 18.187840025789495}\n",
            "Losses {'ner': 18.18800201392632}\n",
            "Losses {'ner': 19.87602406066898}\n",
            "Losses {'ner': 20.03877781173068}\n",
            "Losses {'ner': 24.429811692812645}\n",
            "Losses {'ner': 24.600646349000126}\n",
            "Losses {'ner': 25.56793454508701}\n",
            "Losses {'ner': 29.633728536175877}\n",
            "Losses {'ner': 34.97605812768856}\n",
            "Losses {'ner': 34.98443495039601}\n",
            "Losses {'ner': 37.42053337697644}\n",
            "Losses {'ner': 40.468411523148006}\n",
            "Losses {'ner': 42.475518354445285}\n",
            "Losses {'ner': 44.281244539609816}\n",
            "Losses {'ner': 47.336629615119364}\n",
            "Losses {'ner': 54.399867807469036}\n",
            "Losses {'ner': 54.45820027305808}\n",
            "Losses {'ner': 61.06618678285804}\n",
            "Losses {'ner': 65.31256422999203}\n",
            "Losses {'ner': 67.8822610140115}\n",
            "Losses {'ner': 73.1141875446111}\n",
            "Losses {'ner': 79.14295231404603}\n",
            "Losses {'ner': 79.14314005078874}\n",
            "Losses {'ner': 82.96865764560305}\n",
            "Losses {'ner': 88.72391277544982}\n",
            "Losses {'ner': 92.33806514733321}\n",
            "Losses {'ner': 99.12637256079203}\n",
            "Losses {'ner': 103.51276805990226}\n",
            "Losses {'ner': 108.00954744213111}\n",
            "Losses {'ner': 110.55762584844221}\n",
            "Losses {'ner': 113.3808964246938}\n",
            "Losses {'ner': 114.38934943620656}\n",
            "Losses {'ner': 122.86625498910975}\n",
            "Losses {'ner': 122.86625498910975}\n",
            "Losses {'ner': 124.6122192778213}\n",
            "Losses {'ner': 129.6203666516168}\n",
            "Losses {'ner': 4.816816866397858}\n",
            "Losses {'ner': 9.906550832092762}\n",
            "Losses {'ner': 14.298360091168433}\n",
            "Losses {'ner': 16.23944677898544}\n",
            "Losses {'ner': 21.918774259480415}\n",
            "Losses {'ner': 24.816460448571206}\n",
            "Losses {'ner': 29.59666250899977}\n",
            "Losses {'ner': 31.867844226514762}\n",
            "Losses {'ner': 35.21481879546559}\n",
            "Losses {'ner': 37.92601454369219}\n",
            "Losses {'ner': 38.9001353581823}\n",
            "Losses {'ner': 43.233285700289116}\n",
            "Losses {'ner': 45.85704241817962}\n",
            "Losses {'ner': 46.92818143029376}\n",
            "Losses {'ner': 47.885358771278106}\n",
            "Losses {'ner': 49.94561054368182}\n",
            "Losses {'ner': 52.04457188678293}\n",
            "Losses {'ner': 56.26622481179743}\n",
            "Losses {'ner': 59.09965076890853}\n",
            "Losses {'ner': 60.702227383178155}\n",
            "Losses {'ner': 61.83724493080726}\n",
            "Losses {'ner': 66.64453539544692}\n",
            "Losses {'ner': 66.70590373093756}\n",
            "Losses {'ner': 66.70590373093756}\n",
            "Losses {'ner': 66.95899539405974}\n",
            "Losses {'ner': 72.86213618499488}\n",
            "Losses {'ner': 72.86213618503015}\n",
            "Losses {'ner': 78.52063327070218}\n",
            "Losses {'ner': 80.00336690273458}\n",
            "Losses {'ner': 81.40965918664212}\n",
            "Losses {'ner': 82.66202008191819}\n",
            "Losses {'ner': 88.8101209296167}\n",
            "Losses {'ner': 89.77858494653107}\n",
            "Losses {'ner': 95.24686888291241}\n",
            "Losses {'ner': 97.20978802079505}\n",
            "Losses {'ner': 100.31863838071173}\n",
            "Losses {'ner': 105.31615009540862}\n",
            "Losses {'ner': 115.51756426328963}\n",
            "Losses {'ner': 119.5558735393936}\n",
            "Losses {'ner': 4.895518362522125}\n",
            "Losses {'ner': 15.349955201148987}\n",
            "Losses {'ner': 19.607998073101044}\n",
            "Losses {'ner': 26.571763396263123}\n",
            "Losses {'ner': 35.320694863796234}\n",
            "Losses {'ner': 35.664467903610785}\n",
            "Losses {'ner': 48.87622437143}\n",
            "Losses {'ner': 59.93820289039286}\n",
            "Losses {'ner': 63.886469754215796}\n",
            "Losses {'ner': 69.43980416917475}\n",
            "Losses {'ner': 75.51738153659971}\n",
            "Losses {'ner': 82.56375335418852}\n",
            "Losses {'ner': 88.61549356787191}\n",
            "Losses {'ner': 90.7424906958322}\n",
            "Losses {'ner': 92.07484473254499}\n",
            "Losses {'ner': 92.0748479509889}\n",
            "Losses {'ner': 97.00582320750573}\n",
            "Losses {'ner': 100.97936745495417}\n",
            "Losses {'ner': 109.84717722744563}\n",
            "Losses {'ner': 111.7724282345257}\n",
            "Losses {'ner': 120.8783915242634}\n",
            "Losses {'ner': 125.54848303696372}\n",
            "Losses {'ner': 7.99454802274704}\n",
            "Losses {'ner': 11.997037316672504}\n",
            "Losses {'ner': 15.656121994368732}\n",
            "Losses {'ner': 22.873244966380298}\n",
            "Losses {'ner': 23.87216089235335}\n",
            "Losses {'ner': 23.991452677815122}\n",
            "Losses {'ner': 38.710965140431085}\n",
            "Losses {'ner': 44.449249251454034}\n",
            "Losses {'ner': 52.019823356239954}\n",
            "Losses {'ner': 58.841798170178095}\n",
            "Losses {'ner': 60.7965255640878}\n",
            "Losses {'ner': 66.20317670293969}\n",
            "Losses {'ner': 75.94238730856102}\n",
            "Losses {'ner': 77.21768655380856}\n",
            "Losses {'ner': 81.58695972719852}\n",
            "Losses {'ner': 88.95777726927463}\n",
            "Losses {'ner': 93.73746516941628}\n",
            "Losses {'ner': 100.11697825430474}\n",
            "Losses {'ner': 105.8966480010613}\n",
            "Losses {'ner': 116.49050984917722}\n",
            "Losses {'ner': 117.96716607561848}\n",
            "Losses {'ner': 121.84463269463322}\n",
            "Losses {'ner': 11.866501688957214}\n",
            "Losses {'ner': 18.709145605564117}\n",
            "Losses {'ner': 28.67928570508957}\n",
            "Losses {'ner': 29.656536386624794}\n",
            "Losses {'ner': 30.630396910860604}\n",
            "Losses {'ner': 36.4981320583368}\n",
            "Losses {'ner': 43.454835333778924}\n",
            "Losses {'ner': 44.458793929404514}\n",
            "Losses {'ner': 48.131198285705345}\n",
            "Losses {'ner': 49.148513076649856}\n",
            "Losses {'ner': 54.732623619781684}\n",
            "Losses {'ner': 59.28734086249943}\n",
            "Losses {'ner': 67.40427677129384}\n",
            "Losses {'ner': 70.49401234352018}\n",
            "Losses {'ner': 76.33778141141858}\n",
            "Losses {'ner': 83.58160023094143}\n",
            "Losses {'ner': 86.62521464741195}\n",
            "Losses {'ner': 91.67373787975276}\n",
            "Losses {'ner': 91.67408976892813}\n",
            "Losses {'ner': 101.0044089351116}\n",
            "Losses {'ner': 105.62609242286754}\n",
            "Losses {'ner': 110.73977837350441}\n",
            "Losses {'ner': 7.105206847190857}\n",
            "Losses {'ner': 11.948041851341259}\n",
            "Losses {'ner': 25.407134259759914}\n",
            "Losses {'ner': 32.52634676796151}\n",
            "Losses {'ner': 33.49076460077985}\n",
            "Losses {'ner': 39.6669358886741}\n",
            "Losses {'ner': 42.588568328361625}\n",
            "Losses {'ner': 46.25532059226524}\n",
            "Losses {'ner': 51.39337472472679}\n",
            "Losses {'ner': 53.255863482081885}\n",
            "Losses {'ner': 62.14466124209165}\n",
            "Losses {'ner': 66.73575770291089}\n",
            "Losses {'ner': 72.82789891394376}\n",
            "Losses {'ner': 85.29673557909726}\n",
            "Losses {'ner': 93.82379835280179}\n",
            "Losses {'ner': 93.82379848902853}\n",
            "Losses {'ner': 102.69263071959647}\n",
            "Losses {'ner': 106.97305974105181}\n",
            "Losses {'ner': 113.40933188536944}\n",
            "Losses {'ner': 119.80152795894462}\n",
            "Losses {'ner': 123.66491188807429}\n",
            "Losses {'ner': 126.85889394814164}\n",
            "Losses {'ner': 4.631500510266051}\n",
            "Losses {'ner': 12.952642334392294}\n",
            "Losses {'ner': 21.101931584766135}\n",
            "Losses {'ner': 29.534545792033896}\n",
            "Losses {'ner': 41.07203794806264}\n",
            "Losses {'ner': 45.785244000842795}\n",
            "Losses {'ner': 51.91288865893148}\n",
            "Losses {'ner': 57.22652186243795}\n",
            "Losses {'ner': 68.56070102541707}\n",
            "Losses {'ner': 69.96871420463867}\n",
            "Losses {'ner': 71.74775423600659}\n",
            "Losses {'ner': 77.52559782054595}\n",
            "Losses {'ner': 82.96659547255695}\n",
            "Losses {'ner': 91.5901115255183}\n",
            "Losses {'ner': 94.69228586719692}\n",
            "Losses {'ner': 97.77709604798292}\n",
            "Losses {'ner': 100.39966933427786}\n",
            "Losses {'ner': 106.39828531442618}\n",
            "Losses {'ner': 114.34780792890524}\n",
            "Losses {'ner': 118.88645805417036}\n",
            "Losses {'ner': 125.50920468055119}\n",
            "Losses {'ner': 127.38850268209353}\n",
            "Losses {'ner': 7.853372037410736}\n",
            "Losses {'ner': 14.595522873103619}\n",
            "Losses {'ner': 21.836945943534374}\n",
            "Losses {'ner': 23.852390190877486}\n",
            "Losses {'ner': 27.00326763681369}\n",
            "Losses {'ner': 31.889419799961615}\n",
            "Losses {'ner': 37.68494718364673}\n",
            "Losses {'ner': 46.02059488586383}\n",
            "Losses {'ner': 48.54809277347522}\n",
            "Losses {'ner': 58.03283666662173}\n",
            "Losses {'ner': 67.13873385480838}\n",
            "Losses {'ner': 71.62364937952952}\n",
            "Losses {'ner': 76.66441651276546}\n",
            "Losses {'ner': 82.40649343753466}\n",
            "Losses {'ner': 92.01289356971392}\n",
            "Losses {'ner': 97.00322454149136}\n",
            "Losses {'ner': 101.23410980834433}\n",
            "Losses {'ner': 112.62589864863821}\n",
            "Losses {'ner': 114.58226925122281}\n",
            "Losses {'ner': 115.60373105240242}\n",
            "Losses {'ner': 119.54622137073048}\n",
            "Losses {'ner': 122.90413191441067}\n",
            "Losses {'ner': 10.67365163564682}\n",
            "Losses {'ner': 12.822585873180287}\n",
            "Losses {'ner': 20.15181090670194}\n",
            "Losses {'ner': 23.865816317373174}\n",
            "Losses {'ner': 33.26896136241521}\n",
            "Losses {'ner': 40.34481171506013}\n",
            "Losses {'ner': 50.25756964819993}\n",
            "Losses {'ner': 54.14350829260911}\n",
            "Losses {'ner': 58.144820762323434}\n",
            "Losses {'ner': 61.049281728830465}\n",
            "Losses {'ner': 67.58918048199666}\n",
            "Losses {'ner': 69.15692265928192}\n",
            "Losses {'ner': 71.74900535582674}\n",
            "Losses {'ner': 84.99945227145326}\n",
            "Losses {'ner': 93.23878596811664}\n",
            "Losses {'ner': 96.95864302567315}\n",
            "Losses {'ner': 99.78609052343018}\n",
            "Losses {'ner': 106.25187316579468}\n",
            "Losses {'ner': 107.26929678098895}\n",
            "Losses {'ner': 109.87945486588069}\n",
            "Losses {'ner': 114.76442256133669}\n",
            "Losses {'ner': 116.88598357634184}\n",
            "Losses {'ner': 13.018233895301819}\n",
            "Losses {'ner': 20.893105387687683}\n",
            "Losses {'ner': 27.976212978363037}\n",
            "Losses {'ner': 33.06133636832237}\n",
            "Losses {'ner': 34.179705906659365}\n",
            "Losses {'ner': 39.1340346802026}\n",
            "Losses {'ner': 41.613771927542984}\n",
            "Losses {'ner': 52.79854811448604}\n",
            "Losses {'ner': 58.973097431473434}\n",
            "Losses {'ner': 62.84377898555249}\n",
            "Losses {'ner': 68.99755962472409}\n",
            "Losses {'ner': 69.96400256734341}\n",
            "Losses {'ner': 78.47547205071896}\n",
            "Losses {'ner': 84.94178171735257}\n",
            "Losses {'ner': 87.29470711830072}\n",
            "Losses {'ner': 91.10223930957727}\n",
            "Losses {'ner': 95.71156558490702}\n",
            "Losses {'ner': 97.69441887832825}\n",
            "Losses {'ner': 107.81167814232056}\n",
            "Losses {'ner': 110.87526772264853}\n",
            "Losses {'ner': 120.12051180843726}\n",
            "Losses {'ner': 124.42318163160697}\n",
            "Losses {'ner': 4.254803100135177}\n",
            "Losses {'ner': 12.612039843108505}\n",
            "Losses {'ner': 22.341022768523544}\n",
            "Losses {'ner': 27.209720054175705}\n",
            "Losses {'ner': 32.22596985427663}\n",
            "Losses {'ner': 34.525567909237}\n",
            "Losses {'ner': 39.55385096324608}\n",
            "Losses {'ner': 41.509110596005144}\n",
            "Losses {'ner': 44.65973998917946}\n",
            "Losses {'ner': 57.05245258225807}\n",
            "Losses {'ner': 58.22432789356901}\n",
            "Losses {'ner': 66.91743685455515}\n",
            "Losses {'ner': 73.48254563064768}\n",
            "Losses {'ner': 77.01321934325608}\n",
            "Losses {'ner': 80.07227950705203}\n",
            "Losses {'ner': 89.60591092122706}\n",
            "Losses {'ner': 91.03161372220131}\n",
            "Losses {'ner': 101.40654303109261}\n",
            "Losses {'ner': 106.60903401641337}\n",
            "Losses {'ner': 110.47946371106593}\n",
            "Losses {'ner': 117.31806530026881}\n",
            "Losses {'ner': 118.98242308316333}\n",
            "Losses {'ner': 5.22954795100668}\n",
            "Losses {'ner': 13.273410234236508}\n",
            "Losses {'ner': 15.831423740341506}\n",
            "Losses {'ner': 16.03085225305631}\n",
            "Losses {'ner': 19.960176630151636}\n",
            "Losses {'ner': 26.323177869637675}\n",
            "Losses {'ner': 33.47478174718208}\n",
            "Losses {'ner': 37.47910451443977}\n",
            "Losses {'ner': 41.11396479161567}\n",
            "Losses {'ner': 52.79174953254051}\n",
            "Losses {'ner': 59.32812374385185}\n",
            "Losses {'ner': 66.65259739311523}\n",
            "Losses {'ner': 71.31088719071931}\n",
            "Losses {'ner': 73.75490751044936}\n",
            "Losses {'ner': 79.98518856542296}\n",
            "Losses {'ner': 88.95398791806883}\n",
            "Losses {'ner': 97.92566259877867}\n",
            "Losses {'ner': 97.92566259878376}\n",
            "Losses {'ner': 99.2902474168005}\n",
            "Losses {'ner': 103.43712470561886}\n",
            "Losses {'ner': 113.35984464616203}\n",
            "Losses {'ner': 115.61994995589166}\n",
            "Losses {'ner': 4.279496788978577}\n",
            "Losses {'ner': 6.273868358810432}\n",
            "Losses {'ner': 10.018604645621963}\n",
            "Losses {'ner': 13.653337763767922}\n",
            "Losses {'ner': 17.651898535114015}\n",
            "Losses {'ner': 28.66000530324527}\n",
            "Losses {'ner': 38.17289081416675}\n",
            "Losses {'ner': 38.18658722299733}\n",
            "Losses {'ner': 41.566519354848424}\n",
            "Losses {'ner': 44.70361140873865}\n",
            "Losses {'ner': 52.573212531773606}\n",
            "Losses {'ner': 58.144106646446744}\n",
            "Losses {'ner': 64.53907909002737}\n",
            "Losses {'ner': 67.57149054613546}\n",
            "Losses {'ner': 77.00944884624914}\n",
            "Losses {'ner': 87.21361800041632}\n",
            "Losses {'ner': 88.26743193474249}\n",
            "Losses {'ner': 92.96533091869787}\n",
            "Losses {'ner': 100.33376257625059}\n",
            "Losses {'ner': 111.87087201085524}\n",
            "Losses {'ner': 119.25136849391856}\n",
            "Losses {'ner': 123.53341129343607}\n",
            "Losses {'ner': 10.795449376106262}\n",
            "Losses {'ner': 12.512260172725291}\n",
            "Losses {'ner': 20.337838087440105}\n",
            "Losses {'ner': 28.988464657426448}\n",
            "Losses {'ner': 39.28716910278854}\n",
            "Losses {'ner': 44.52580470240173}\n",
            "Losses {'ner': 44.52622603277541}\n",
            "Losses {'ner': 50.423178363841544}\n",
            "Losses {'ner': 57.044064808887015}\n",
            "Losses {'ner': 67.93587248424865}\n",
            "Losses {'ner': 69.91908580052677}\n",
            "Losses {'ner': 71.05650190810684}\n",
            "Losses {'ner': 77.03469781379226}\n",
            "Losses {'ner': 79.92847346611961}\n",
            "Losses {'ner': 86.62798022576317}\n",
            "Losses {'ner': 88.13209264147736}\n",
            "Losses {'ner': 88.21719973241397}\n",
            "Losses {'ner': 96.70909207021305}\n",
            "Losses {'ner': 100.27003150140354}\n",
            "Losses {'ner': 112.0742668703706}\n",
            "Losses {'ner': 114.45516871647396}\n",
            "Losses {'ner': 114.5287091418034}\n",
            "Losses {'ner': 10.293221175670624}\n",
            "Losses {'ner': 14.33753487560898}\n",
            "Losses {'ner': 20.19830593559891}\n",
            "Losses {'ner': 21.254759976270407}\n",
            "Losses {'ner': 31.021932074430197}\n",
            "Losses {'ner': 36.01617282778284}\n",
            "Losses {'ner': 36.859502930755525}\n",
            "Losses {'ner': 43.90119248312226}\n",
            "Losses {'ner': 45.87872185705555}\n",
            "Losses {'ner': 52.962546372398265}\n",
            "Losses {'ner': 57.222060864403375}\n",
            "Losses {'ner': 63.7150258958186}\n",
            "Losses {'ner': 70.01732580720773}\n",
            "Losses {'ner': 72.92361013958407}\n",
            "Losses {'ner': 77.98009380320264}\n",
            "Losses {'ner': 85.62717631081296}\n",
            "Losses {'ner': 88.11208959290934}\n",
            "Losses {'ner': 95.75208138296557}\n",
            "Losses {'ner': 99.16539037594748}\n",
            "Losses {'ner': 100.15080001743507}\n",
            "Losses {'ner': 106.15765711458397}\n",
            "Losses {'ner': 106.16391901408451}\n",
            "Losses {'ner': 4.6403756737709045}\n",
            "Losses {'ner': 7.201297619845718}\n",
            "Losses {'ner': 9.429596411926468}\n",
            "Losses {'ner': 14.705696689468596}\n",
            "Losses {'ner': 26.516927706581328}\n",
            "Losses {'ner': 34.55427251992819}\n",
            "Losses {'ner': 38.44987768276201}\n",
            "Losses {'ner': 43.50962797148691}\n",
            "Losses {'ner': 46.648067340758416}\n",
            "Losses {'ner': 50.55185292711553}\n",
            "Losses {'ner': 53.78447282355933}\n",
            "Losses {'ner': 58.407343267850365}\n",
            "Losses {'ner': 69.8914394373221}\n",
            "Losses {'ner': 69.90764259102593}\n",
            "Losses {'ner': 71.86545644264856}\n",
            "Losses {'ner': 79.01704651799241}\n",
            "Losses {'ner': 86.41819382395784}\n",
            "Losses {'ner': 91.25862377073804}\n",
            "Losses {'ner': 96.38978048828153}\n",
            "Losses {'ner': 99.05842098262815}\n",
            "Losses {'ner': 101.60988062182855}\n",
            "Losses {'ner': 106.09336634468468}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Zca9ZRyX30"
      },
      "source": [
        "Demonstrating the performance of the NER using an example from the initial string list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2ivN1G4JVNeX",
        "outputId": "f8d2a0ce-f921-44d4-b0a4-25dcee650fa1"
      },
      "source": [
        "doc = nlp(string_list[0]) \n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    marks and spencer ltd\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRpucX1YgPCO"
      },
      "source": [
        "**Serial Numbers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoiZXazmyesM"
      },
      "source": [
        "I decided to go with a Spacy Matcher for the Serial Numbers due to their repetitive nature that can be easily predicted using regular expressions. \n",
        "\n",
        "To finalise the model, this would need to be integrated into the flow of events. Alternatively, we could train the NER to detect the ID numbers as a unique entity and we could check these against an EntityLinker, in the same way as the other entites. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZCH0M-y1kL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026a2d5e-446b-45e6-b7e4-128f6ce58e73"
      },
      "source": [
        "import re\n",
        "doc = nlp(\"XYZ 13423 / ILD, ABC/ICL/20891NC\")\n",
        "#doc = nlp(string_list[2])\n",
        "print([t.text for t in doc])\n",
        "\n",
        "serial_num_matches = []\n",
        "\n",
        "expressions = [r\"[A-Za-z]{3,}[/]?[A-Za-z]{3,}[/]?[0-9]{1,}[A-Za-z]+\",\n",
        "               r\"[A-Za-z]{3,}[\\s]?[/]?[0-9]+[\\s]?[/]?[\\s]?[A-Za-z]{3,}\",\n",
        "               r\"[A-Za-z][0-9]+\"]\n",
        "\n",
        "for expression in expressions:\n",
        "  for match in re.finditer(expression, doc.text):\n",
        "      print(match)\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end)\n",
        "      # This is a Span object or None if match doesn't map to valid token sequence\n",
        "      if span is not None:\n",
        "          print(\"Found match:\", span.text)\n",
        "          span_text = span.text\n",
        "          string_a = span_text.replace(\"/\",\" \")\n",
        "          print(string_a)\n",
        "          string_b = string_a.replace(\" \",\"\")\n",
        "          print(\"replaced string:\")\n",
        "          print(string_b)\n",
        "          if string_b not in serial_num_matches:\n",
        "              serial_num_matches.append(string_b)\n",
        "print(serial_num_matches)"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['XYZ', '13423', '/', 'ILD', ',', 'ABC', '/', 'ICL/20891NC']\n",
            "<re.Match object; span=(17, 32), match='ABC/ICL/20891NC'>\n",
            "Found match: ABC/ICL/20891NC\n",
            "ABC ICL 20891NC\n",
            "replaced string:\n",
            "ABCICL20891NC\n",
            "<re.Match object; span=(0, 15), match='XYZ 13423 / ILD'>\n",
            "Found match: XYZ 13423 / ILD\n",
            "XYZ 13423   ILD\n",
            "replaced string:\n",
            "XYZ13423ILD\n",
            "['ABCICL20891NC', 'XYZ13423ILD']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tkWlbbLcFNm"
      },
      "source": [
        "# Entity Linking\n",
        "\n",
        "This next step focuses on the alphanumeric entities that have been trained previously using the EntityRuler. \n",
        "\n",
        "We're going to setup an EntityLinker which allows us to match entities with unique ID's and also teaches the model to decipher between synonyms of the same entity. \n",
        "The below EntityLinker for example, will match the entities \"M&S\" and \"Marks and Spencer\" with the entity \"Marks & Spencer\" with ID code Q714491. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4vxqr4cYOP"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def load_entities():\n",
        "    entities_loc = Path.cwd().parent / \"/entities_aliases.csv\"  # distributed alongside this notebook\n",
        "\n",
        "    names = dict()\n",
        "    descriptions = dict()\n",
        "    aliases = dict()\n",
        "    with entities_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
        "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
        "        for row in csvreader:\n",
        "            qid = row[0]\n",
        "            print(qid)\n",
        "            name = row[1]\n",
        "            print(name)\n",
        "            desc = row[2]\n",
        "            aliases_list = row[3:]\n",
        "            print(aliases_list)\n",
        "            names[qid] = name\n",
        "            descriptions[qid] = desc\n",
        "            aliases[qid] = aliases_list\n",
        "    return names, descriptions, aliases"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GnYi9iMy-gJ"
      },
      "source": [
        "Adding entity aliases from file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXO3PFiPc1qi",
        "outputId": "e6bac310-92ea-456d-ddc2-15c09695190a"
      },
      "source": [
        "name_dict, desc_dict, alias_dict = load_entities()\n",
        "for QID in name_dict.keys():\n",
        "    print(f\"{QID}, name={name_dict[QID]}, desc={desc_dict[QID]}, aliases={alias_dict[QID]}\")\n",
        "\n"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q714491\n",
            "Marks & Spencer\n",
            "['m&s', 'marks and spencer limited', 'marks and spencer ltd', 'marks and spencer', 'm&s corporation limited']\n",
            "Q182477\n",
            "Nvidia\n",
            "['nvidia ireland', 'nvidia corporation']\n",
            "Q248\n",
            "Intel\n",
            "['intel corp', 'intel corporation', 'integrated electronics', 'intel llc']\n",
            "Q84\n",
            "London\n",
            "['london', 'london, great britain', 'london, england']\n",
            "Q48\n",
            "Asia\n",
            "['east asia', 'south asia']\n",
            "P1\n",
            "hardwood table\n",
            "['wooden table', 'wood table']\n",
            "P2\n",
            "plastic chair\n",
            "[]\n",
            "P3\n",
            "coffee table\n",
            "['side table']\n",
            "P4\n",
            "plastic bottle\n",
            "['single use bottle']\n",
            "P5\n",
            "fillet knife\n",
            "['fillet blade']\n",
            "P6\n",
            "toy\n",
            "['toys']\n",
            "Q714491, name=Marks & Spencer, desc=retail company, aliases=['m&s', 'marks and spencer limited', 'marks and spencer ltd', 'marks and spencer', 'm&s corporation limited']\n",
            "Q182477, name=Nvidia, desc=American multinational technology company, aliases=['nvidia ireland', 'nvidia corporation']\n",
            "Q248, name=Intel, desc=American semiconductor chip manufacturer, aliases=['intel corp', 'intel corporation', 'integrated electronics', 'intel llc']\n",
            "Q84, name=London, desc=capital and largest city of the United Kingdom, aliases=['london', 'london, great britain', 'london, england']\n",
            "Q48, name=Asia, desc=continent on Earth, mainly on the Earth's northeastern quadrant, aliases=['east asia', 'south asia']\n",
            "P1, name=hardwood table, desc=piece of hardwood furniture with a flat top, aliases=['wooden table', 'wood table']\n",
            "P2, name=plastic chair, desc=chair constructed of plastic, aliases=[]\n",
            "P3, name=coffee table, desc=short table for use in a living room, aliases=['side table']\n",
            "P4, name=plastic bottle, desc=bottle constructed of plastic, aliases=['single use bottle']\n",
            "P5, name=fillet knife, desc=knife for filliting, aliases=['fillet blade']\n",
            "P6, name=toy, desc=object intended to be played with, aliases=['toys']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqqFycA3zJmW"
      },
      "source": [
        "Creating a knowledge base:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DBJHHDelHn"
      },
      "source": [
        "from spacy.kb import KnowledgeBase\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=96)"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag6uDgavzMIK"
      },
      "source": [
        "Adding entities to knowledge base:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAmyEXEfewtV"
      },
      "source": [
        "for qid, desc in desc_dict.items():\n",
        "    desc_doc = nlp(desc)\n",
        "    desc_enc = desc_doc.vector\n",
        "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)   \n",
        "    # 342 is an arbitrary value here"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvMnuM-QzSIk"
      },
      "source": [
        "Adding aliases for each entity, with a probability of 100%. If we had an entity that could potentially be matched with more than 1 alias then we can add these in here with the appropriate probabilities. E.g. if you had 3 entities for 1 alias then the probabilities would be [0.3,0.3,0.3]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMbtmSdPgVpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99ebe57-9d4e-42ac-fbaa-b119d44c7c47"
      },
      "source": [
        "for qid, name in name_dict.items():\n",
        "  print(qid,name)\n",
        "  kb.add_alias(alias=name, entities=[qid], probabilities=[1])   # 100% prior probability P(entity|alias)"
      ],
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q714491 Marks & Spencer\n",
            "Q182477 Nvidia\n",
            "Q248 Intel\n",
            "Q84 London\n",
            "Q48 Asia\n",
            "P1 hardwood table\n",
            "P2 plastic chair\n",
            "P3 coffee table\n",
            "P4 plastic bottle\n",
            "P5 fillet knife\n",
            "P6 toy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbqWNBpX1964"
      },
      "source": [
        "Adding Aliases for each unique cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iebBKZpM3nNP",
        "outputId": "33b5948f-8f52-41d9-e465-802231ea9504"
      },
      "source": [
        "#print(alias_dict)\n",
        "#print(alias_dict.values())\n",
        "\n",
        "for qid, alias in alias_dict.items():\n",
        "  for synm in alias:\n",
        "    print(\"Alias no: \", qid)\n",
        "    print(\"Alias: \", synm)\n",
        "    print(\"\")\n",
        "\n",
        "    kb.add_alias(alias=synm, entities=[qid], probabilities=[1])  # sum([probs]) should be <= 1 !"
      ],
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alias no:  Q714491\n",
            "Alias:  m&s\n",
            "\n",
            "Alias no:  Q714491\n",
            "Alias:  marks and spencer limited\n",
            "\n",
            "Alias no:  Q714491\n",
            "Alias:  marks and spencer ltd\n",
            "\n",
            "Alias no:  Q714491\n",
            "Alias:  marks and spencer\n",
            "\n",
            "Alias no:  Q714491\n",
            "Alias:  m&s corporation limited\n",
            "\n",
            "Alias no:  Q182477\n",
            "Alias:  nvidia ireland\n",
            "\n",
            "Alias no:  Q182477\n",
            "Alias:  nvidia corporation\n",
            "\n",
            "Alias no:  Q248\n",
            "Alias:  intel corp\n",
            "\n",
            "Alias no:  Q248\n",
            "Alias:  intel corporation\n",
            "\n",
            "Alias no:  Q248\n",
            "Alias:  integrated electronics\n",
            "\n",
            "Alias no:  Q248\n",
            "Alias:  intel llc\n",
            "\n",
            "Alias no:  Q84\n",
            "Alias:  london\n",
            "\n",
            "Alias no:  Q84\n",
            "Alias:  london, great britain\n",
            "\n",
            "Alias no:  Q84\n",
            "Alias:  london, england\n",
            "\n",
            "Alias no:  Q48\n",
            "Alias:  east asia\n",
            "\n",
            "Alias no:  Q48\n",
            "Alias:  south asia\n",
            "\n",
            "Alias no:  P1\n",
            "Alias:  wooden table\n",
            "\n",
            "Alias no:  P1\n",
            "Alias:  wood table\n",
            "\n",
            "Alias no:  P3\n",
            "Alias:  side table\n",
            "\n",
            "Alias no:  P4\n",
            "Alias:  single use bottle\n",
            "\n",
            "Alias no:  P5\n",
            "Alias:  fillet blade\n",
            "\n",
            "Alias no:  P6\n",
            "Alias:  toys\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvcaJ0B42C6l",
        "outputId": "7541a7dd-d8e9-443a-ab01-6d90473b9394"
      },
      "source": [
        "print(name_dict)"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Q714491': 'Marks & Spencer', 'Q182477': 'Nvidia', 'Q248': 'Intel', 'Q84': 'London', 'Q48': 'Asia', 'P1': 'hardwood table', 'P2': 'plastic chair', 'P3': 'coffee table', 'P4': 'plastic bottle', 'P5': 'fillet knife', 'P6': 'toy'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSNKu_JbwR5x"
      },
      "source": [
        "Taking a look at the different cluster ID's. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_4jWxuxglUt",
        "outputId": "0bb9d6b7-79ff-41db-fe99-061853bf4b07"
      },
      "source": [
        "qids = name_dict.keys()\n",
        "for qid in qids:\n",
        "  print(qid)"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q714491\n",
            "Q182477\n",
            "Q248\n",
            "Q84\n",
            "Q48\n",
            "P1\n",
            "P2\n",
            "P3\n",
            "P4\n",
            "P5\n",
            "P6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve9sgGe9ndAL",
        "outputId": "a0c0c846-484a-4e66-d9f9-d5b56f6f467a"
      },
      "source": [
        "print(len(kb))\n",
        "print(kb.get_entity_strings())"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "['Q248', 'P1', 'P2', 'Q714491', 'P6', 'P5', 'Q48', 'Q84', 'P4', 'Q182477', 'P3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HBZmCq5gvv2",
        "outputId": "b1501e74-6add-4982-b3ad-21b7b954e0b6"
      },
      "source": [
        "print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n",
        "print(f\"Aliases in the KB: {kb.get_alias_strings()}\")"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities in the KB: ['Q248', 'P1', 'P2', 'Q714491', 'P6', 'P5', 'Q48', 'Q84', 'P4', 'Q182477', 'P3']\n",
            "Aliases in the KB: ['intel llc', 'nvidia corporation', 'Nvidia', 'east asia', 'london', 'Asia', 'coffee table', 'm&s corporation limited', 'London', 'south asia', 'integrated electronics', 'wooden table', 'london, england', 'm&s', 'toy', 'plastic chair', 'intel corp', 'single use bottle', 'Marks & Spencer', 'fillet knife', 'Intel', 'marks and spencer ltd', 'toys', 'intel corporation', 'fillet blade', 'marks and spencer limited', 'marks and spencer', 'nvidia ireland', 'side table', 'hardwood table', 'plastic bottle', 'wood table', 'london, great britain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCxa-7FD4YkH"
      },
      "source": [
        "Checking some of the ID candidates for  entity strings the EntityLinker expects, and one extra entity that it won't recognise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o73FkOWg4AA",
        "outputId": "e976c1ce-2297-4107-c982-bb90966e19ff"
      },
      "source": [
        "print(f\"Candidates for 'London': {[c.entity_ for c in kb.get_candidates('London')]}\")\n",
        "print(f\"Candidates for 'plastic chair': {[c.entity_ for c in kb.get_candidates('plastic chair')]}\")\n",
        "print(f\"Candidates for 'Intel': {[c.entity_ for c in kb.get_candidates('Intel')]}\")\n",
        "print(f\"Candidates for 'Asia': {[c.entity_ for c in kb.get_candidates('Asia')]}\")\n",
        "print(f\"Candidates for 'Australia': {[c.entity_ for c in kb.get_candidates('Australia')]}\")\n"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Candidates for 'London': ['Q84']\n",
            "Candidates for 'plastic chair': ['P2']\n",
            "Candidates for 'Intel': ['Q248']\n",
            "Candidates for 'Asia': ['Q48']\n",
            "Candidates for 'Australia': []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-rQijO4qyG"
      },
      "source": [
        "Saving knowledge base and nlp model to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlOI39yWrm6-"
      },
      "source": [
        "import os\n",
        "output_dir = Path.cwd().parent / \"my_output\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir) \n",
        "kb.dump(output_dir / \"my_kb\")"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_evBbImrx6B"
      },
      "source": [
        "nlp.to_disk(output_dir / \"my_nlp\")"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbGvdjjX4xJu"
      },
      "source": [
        "Loading in manual annotations. This could be done alternatively by using a custom Prodigy recipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTR2ivXh5DvO"
      },
      "source": [
        "1. Loading company annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Y9EphBZfi1",
        "outputId": "01498f64-ce09-48fc-8d5a-9de8397e83c1"
      },
      "source": [
        "annotations_loc = Path.cwd().parent / \"/company_input.csv\"\n",
        "\n",
        "dataset = []\n",
        "with annotations_loc.open(\"r\", encoding=\"utf8\") as annofile:\n",
        "    csvreader = csv.reader(annofile, delimiter=\",\")\n",
        "    for row in csvreader:\n",
        "        #print(row)\n",
        "        text = row[0]\n",
        "        answer = row[6]\n",
        "        if answer == \"accept\":\n",
        "            QID = row[5]\n",
        "            offset = (int(row[1]), int(row[2]))\n",
        "            links_dict = {QID: 1.0}\n",
        "        dataset.append((text, {\"links\": {offset: links_dict}}))\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "  print(dataset[i])"
      ],
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}})\n",
            "('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}})\n",
            "('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}})\n",
            "('marks and spencer corporation limited', {'links': {(0, 33): {'Q714491': 1.0}}})\n",
            "('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}})\n",
            "('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}})\n",
            "('intel corp', {'links': {(0, 10): {'Q248': 1.0}}})\n",
            "('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}})\n",
            "('intel llc', {'links': {(0, 9): {'Q248': 1.0}}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgDXwxuu5J0N"
      },
      "source": [
        "2. Loading location annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBUoT_Xm30LR",
        "outputId": "cb1b61ad-ba3a-4a01-bfd5-a7c3457db98c"
      },
      "source": [
        "location_annotations_loc = Path.cwd().parent / \"/locations_input.csv\"  # distributed alongside this notebook\n",
        "\n",
        "with location_annotations_loc.open(\"r\", encoding=\"utf8\") as annofile:\n",
        "    csvreader = csv.reader(annofile, delimiter=\",\")\n",
        "    for row in csvreader:\n",
        "        #print(row)\n",
        "        text = row[0]\n",
        "        answer = row[6]\n",
        "        if answer == \"accept\":\n",
        "            QID = row[5]\n",
        "            offset = (int(row[1]), int(row[2]))\n",
        "            links_dict = {QID: 1.0}\n",
        "        dataset.append((text, {\"links\": {offset: links_dict}}))\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "  print(dataset[i])"
      ],
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}})\n",
            "('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}})\n",
            "('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}})\n",
            "('marks and spencer corporation limited', {'links': {(0, 33): {'Q714491': 1.0}}})\n",
            "('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}})\n",
            "('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}})\n",
            "('intel corp', {'links': {(0, 10): {'Q248': 1.0}}})\n",
            "('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}})\n",
            "('intel llc', {'links': {(0, 9): {'Q248': 1.0}}})\n",
            "('london', {'links': {(0, 6): {'Q84': 1.0}}})\n",
            "('london, england', {'links': {(0, 15): {'Q84': 1.0}}})\n",
            "('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}})\n",
            "('asia', {'links': {(0, 4): {'Q48': 1.0}}})\n",
            "('south asia', {'links': {(0, 10): {'Q48': 1.0}}})\n",
            "('east asia', {'links': {(0, 9): {'Q48': 1.0}}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK0Vyl295N2J"
      },
      "source": [
        "3. Loading in product annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl6Ox3wu4fX1",
        "outputId": "f8cfbb98-7642-4044-bf56-b2c623dd0798"
      },
      "source": [
        "product_annotations_loc = Path.cwd().parent / \"/product_input.csv\"  # distributed alongside this notebook\n",
        "\n",
        "with product_annotations_loc.open(\"r\", encoding=\"utf8\") as annofile:\n",
        "    csvreader = csv.reader(annofile, delimiter=\",\")\n",
        "    for row in csvreader:\n",
        "        #print(row)\n",
        "        text = row[0]\n",
        "        answer = row[6]\n",
        "        if answer == \"accept\":\n",
        "            QID = row[5]\n",
        "            offset = (int(row[1]), int(row[2]))\n",
        "            links_dict = {QID: 1.0}\n",
        "        dataset.append((text, {\"links\": {offset: links_dict}}))\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "  print(dataset[i])"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}})\n",
            "('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}})\n",
            "('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}})\n",
            "('marks and spencer corporation limited', {'links': {(0, 33): {'Q714491': 1.0}}})\n",
            "('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}})\n",
            "('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}})\n",
            "('intel corp', {'links': {(0, 10): {'Q248': 1.0}}})\n",
            "('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}})\n",
            "('intel llc', {'links': {(0, 9): {'Q248': 1.0}}})\n",
            "('london', {'links': {(0, 6): {'Q84': 1.0}}})\n",
            "('london, england', {'links': {(0, 15): {'Q84': 1.0}}})\n",
            "('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}})\n",
            "('asia', {'links': {(0, 4): {'Q48': 1.0}}})\n",
            "('south asia', {'links': {(0, 10): {'Q48': 1.0}}})\n",
            "('east asia', {'links': {(0, 9): {'Q48': 1.0}}})\n",
            "('hardwood table', {'links': {(0, 14): {'P1': 1.0}}})\n",
            "('plastic chair', {'links': {(0, 13): {'P2': 1.0}}})\n",
            "('coffee table', {'links': {(0, 12): {'P3': 1.0}}})\n",
            "('plastic bottle', {'links': {(0, 14): {'P4': 1.0}}})\n",
            "('fillet knife', {'links': {(0, 12): {'P5': 1.0}}})\n",
            "('toys', {'links': {(0, 4): {'P6': 1.0}}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w3rJq2r5Vq4"
      },
      "source": [
        "Setting the \"gold\" ID's and counting how many are in each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMkRyIY0gfrU",
        "outputId": "d8c7b3f8-5ff8-4cdf-b56c-cccd45f30ba1"
      },
      "source": [
        "gold_ids = []\n",
        "for text, annot in dataset:\n",
        "    for span, links_dict in annot[\"links\"].items():\n",
        "        for link, value in links_dict.items():\n",
        "            if value:\n",
        "                gold_ids.append(link)\n",
        "\n",
        "from collections import Counter\n",
        "print(Counter(gold_ids))"
      ],
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'Q714491': 4, 'Q248': 3, 'Q84': 3, 'Q48': 3, 'Q182477': 2, 'P1': 1, 'P2': 1, 'P3': 1, 'P4': 1, 'P5': 1, 'P6': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XaZ2OsnCP0x"
      },
      "source": [
        "Creating test and train datasets. Given the largest category has 3 unique ID's, 2/3's of the ID's will be the train set, and 1/3 will be the test. For the categories with only 1 unique value, this means there is no test for these particular ID's. This could easily be improved by adding more aliases for the undersampled ID. Without accounting for the unequal distribution of ID's, we'll likely encounter overfitting within the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYb-hh3SgkJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0480d0-55df-4a13-8cbf-0967d29421cb"
      },
      "source": [
        "import random\n",
        "\n",
        "train_dataset = []\n",
        "test_dataset = []\n",
        "print(qids)\n",
        "for QID in qids:\n",
        "    indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
        "    len(indices)\n",
        "    train_dataset.extend(dataset[index] for index in indices[0:2])\n",
        "    print(train_dataset)  # first 8 in training\n",
        "    test_dataset.extend(dataset[index] for index in indices[2:3])\n",
        "    print(test_dataset)\n",
        "    # last 2 in test\n",
        "    \n",
        "random.shuffle(train_dataset)\n",
        "random.shuffle(test_dataset)"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['Q714491', 'Q182477', 'Q248', 'Q84', 'Q48', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6'])\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}}), ('hardwood table', {'links': {(0, 14): {'P1': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}}), ('hardwood table', {'links': {(0, 14): {'P1': 1.0}}}), ('plastic chair', {'links': {(0, 13): {'P2': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}}), ('hardwood table', {'links': {(0, 14): {'P1': 1.0}}}), ('plastic chair', {'links': {(0, 13): {'P2': 1.0}}}), ('coffee table', {'links': {(0, 12): {'P3': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}}), ('hardwood table', {'links': {(0, 14): {'P1': 1.0}}}), ('plastic chair', {'links': {(0, 13): {'P2': 1.0}}}), ('coffee table', {'links': {(0, 12): {'P3': 1.0}}}), ('plastic bottle', {'links': {(0, 14): {'P4': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}}), ('hardwood table', {'links': {(0, 14): {'P1': 1.0}}}), ('plastic chair', {'links': {(0, 13): {'P2': 1.0}}}), ('coffee table', {'links': {(0, 12): {'P3': 1.0}}}), ('plastic bottle', {'links': {(0, 14): {'P4': 1.0}}}), ('fillet knife', {'links': {(0, 12): {'P5': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n",
            "[('marks and spencer limited', {'links': {(0, 25): {'Q714491': 1.0}}}), ('marks and spencer', {'links': {(0, 17): {'Q714491': 1.0}}}), ('nvidia', {'links': {(0, 6): {'Q182477': 1.0}}}), ('nvidia ireland', {'links': {(0, 14): {'Q182477': 1.0}}}), ('intel corp', {'links': {(0, 10): {'Q248': 1.0}}}), ('intel corporation', {'links': {(0, 17): {'Q248': 1.0}}}), ('london', {'links': {(0, 6): {'Q84': 1.0}}}), ('london, england', {'links': {(0, 15): {'Q84': 1.0}}}), ('asia', {'links': {(0, 4): {'Q48': 1.0}}}), ('south asia', {'links': {(0, 10): {'Q48': 1.0}}}), ('hardwood table', {'links': {(0, 14): {'P1': 1.0}}}), ('plastic chair', {'links': {(0, 13): {'P2': 1.0}}}), ('coffee table', {'links': {(0, 12): {'P3': 1.0}}}), ('plastic bottle', {'links': {(0, 14): {'P4': 1.0}}}), ('fillet knife', {'links': {(0, 12): {'P5': 1.0}}}), ('toys', {'links': {(0, 4): {'P6': 1.0}}})]\n",
            "[('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_fR0MvVwBPY"
      },
      "source": [
        "Creating train dataset based on nlp entity annotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBi5qbk2goFp"
      },
      "source": [
        "TRAIN_DOCS = []\n",
        "for text, annotation in train_dataset:\n",
        "    doc = nlp(text)     # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
        "    TRAIN_DOCS.append((doc, annotation))"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QB28z6xlRw7",
        "outputId": "a8942cc4-fc75-49d2-912c-f670a11a8223"
      },
      "source": [
        "print(TRAIN_DOCS)\n",
        "print(\"\")\n",
        "print(test_dataset)"
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(coffee table, {'links': {(0, 12): {'P3': 1.0}}}), (nvidia, {'links': {(0, 6): {'Q182477': 1.0}}}), (fillet knife, {'links': {(0, 12): {'P5': 1.0}}}), (asia, {'links': {(0, 4): {'Q48': 1.0}}}), (hardwood table, {'links': {(0, 14): {'P1': 1.0}}}), (plastic chair, {'links': {(0, 13): {'P2': 1.0}}}), (toys, {'links': {(0, 4): {'P6': 1.0}}}), (plastic bottle, {'links': {(0, 14): {'P4': 1.0}}}), (south asia, {'links': {(0, 10): {'Q48': 1.0}}}), (marks and spencer, {'links': {(0, 17): {'Q714491': 1.0}}}), (intel corp, {'links': {(0, 10): {'Q248': 1.0}}}), (london, england, {'links': {(0, 15): {'Q84': 1.0}}}), (nvidia ireland, {'links': {(0, 14): {'Q182477': 1.0}}}), (london, {'links': {(0, 6): {'Q84': 1.0}}}), (marks and spencer limited, {'links': {(0, 25): {'Q714491': 1.0}}}), (intel corporation, {'links': {(0, 17): {'Q248': 1.0}}})]\n",
            "\n",
            "[('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpFKpJM5v3C5"
      },
      "source": [
        "Creating an EntityLinker and setting the knowledge base to the one that's just been created. Adding the entity linker to the nlp. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e8V9_fmKo0O"
      },
      "source": [
        "entity_linker = nlp.create_pipe(\"entity_linker\", config={\"incl_prior\": True})\n",
        "entity_linker.set_kb(kb)\n",
        "nlp.add_pipe(entity_linker, last=True)"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OSD8aqWNngx"
      },
      "source": [
        "entity_linker.set_kb(kb)"
      ],
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbkCKazuvpjz"
      },
      "source": [
        "Training the EntityLinker for 500 iterations with the training dataset which has been pre-formatted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb6zw9LZgyWN",
        "outputId": "a4c4ad79-2300-43ae-bb6c-6f5407f0fccd"
      },
      "source": [
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"entity_linker\"]\n",
        "with nlp.disable_pipes(*other_pipes):   # train only the entity_linker\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(500):   # 500 iterations takes about a minute to train\n",
        "        random.shuffle(TRAIN_DOCS)\n",
        "        batches = minibatch(TRAIN_DOCS, size=compounding(1.0, 4.0, 1.001))  # increasing batch sizes\n",
        "        losses = {}\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "        if itn % 50 == 0:\n",
        "            print(itn, \"Losses\", losses)   # print the training loss\n",
        "print(itn, \"Losses\", losses)"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Losses {'entity_linker': 13.338062703609467}\n",
            "50 Losses {'entity_linker': 0.8009167313575745}\n",
            "100 Losses {'entity_linker': 0.42621612548828125}\n",
            "150 Losses {'entity_linker': 0.38456714153289795}\n",
            "200 Losses {'entity_linker': 0.19490379095077515}\n",
            "250 Losses {'entity_linker': 0.19539499282836914}\n",
            "300 Losses {'entity_linker': 0.16055577993392944}\n",
            "350 Losses {'entity_linker': 0.15167778730392456}\n",
            "400 Losses {'entity_linker': 0.10390567779541016}\n",
            "450 Losses {'entity_linker': 0.08823776245117188}\n",
            "499 Losses {'entity_linker': 0.06734639406204224}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLS5TJqQvVfj"
      },
      "source": [
        "Doing an initial test with the list of strings provided at the very beginning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPdPw9yrTmEc",
        "outputId": "13cf8806-740a-4e07-b2ac-5efadae24225"
      },
      "source": [
        "for i in string_list:\n",
        "    doc = nlp(i)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.label_, ent.kb_id_)"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marks and spencer ltd ORG Q714491\n",
            "london LOC Q84\n",
            "icnao02312 ORG NIL\n",
            "london, great LOC NIL\n",
            "toys LOC P6\n",
            "intel llc LOC Q248\n",
            "london, england LOC Q84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s539LSBjUYxD",
        "outputId": "25cd7d07-04af-4ed0-f823-1322ae7c0843"
      },
      "source": [
        "print(test_dataset)"
      ],
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('intel llc', {'links': {(0, 9): {'Q248': 1.0}}}), ('london, great britain', {'links': {(0, 21): {'Q84': 1.0}}}), ('marks and spencer ltd', {'links': {(0, 21): {'Q714491': 1.0}}}), ('east asia', {'links': {(0, 9): {'Q48': 1.0}}})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEp5SCk-uHXJ"
      },
      "source": [
        "# Testing on the final test dataset.\n",
        "\n",
        "Creating a dictionary of lists for the final unique entity clusters. \n",
        "\n",
        "Each key corresponds to a unique entity (e.g. Marks and Spencer), and the value corresponds to a list of all the aliases found for that entity (e.g. M&S, Marks & Spencer Ltd etc.)\n",
        "\n",
        "The next code cell is for development purposes and wouldn't be used in the final production ready model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDe2kvfMUHLL",
        "outputId": "f29e0910-252c-4059-c943-93cb17c7d3cc"
      },
      "source": [
        "unique_entity_clusters = {} \n",
        "for text, true_annot in test_dataset:\n",
        "    print(\"String \", text)\n",
        "    print(f\"Gold annotation: {true_annot}\")\n",
        "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
        "        if ent.kb_id_ in unique_entity_clusters:\n",
        "            \"Unique Cluster exists\"\n",
        "            if ent.text in unique_entity_clusters[ent.kb_id_]:\n",
        "              print(\"Entity already exists\")\n",
        "              print(\"\")\n",
        "            else:\n",
        "              unique_entity_clusters[ent.kb_id_].append(ent.text)\n",
        "              print(\"\")\n",
        "        else:\n",
        "            print(\"Unique cluster does not exist.\")\n",
        "            unique_entity_clusters[ent.kb_id_] = [ent.text]\n",
        "            print(\"Added unique cluster and entity.\")\n",
        "            print(\"\")\n",
        "\n",
        "print(\"The final unique entity clusters are:\")\n",
        "print(unique_entity_clusters)\n"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "String  intel llc\n",
            "Gold annotation: {'links': {(0, 9): {'Q248': 1.0}}}\n",
            "Prediction: intel llc, LOC, Q248\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  london, great britain\n",
            "Gold annotation: {'links': {(0, 21): {'Q84': 1.0}}}\n",
            "Prediction: london, great, LOC, NIL\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  marks and spencer ltd\n",
            "Gold annotation: {'links': {(0, 21): {'Q714491': 1.0}}}\n",
            "Prediction: marks and spencer ltd, ORG, Q714491\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  east asia\n",
            "Gold annotation: {'links': {(0, 9): {'Q48': 1.0}}}\n",
            "Prediction: east asia, LOC, Q48\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "The final unique entity clusters are:\n",
            "{'Q248': ['intel llc'], 'NIL': ['london, great'], 'Q714491': ['marks and spencer ltd'], 'Q48': ['east asia']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlOU8NTut5Kp"
      },
      "source": [
        "Proof that a new unique is added to the correct cluster if the cluster already exists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuBtLJP4qCR6",
        "outputId": "3ddaf350-7d2a-4d39-aa72-1e41aa5ba612"
      },
      "source": [
        "if \"Q248\" in unique_entity_clusters:\n",
        "  print(\"Unique Cluster exists\")\n",
        "  if \"intel corp\" in unique_entity_clusters[\"Q248\"]:\n",
        "    print(\"Entity already exists\")\n",
        "  else:\n",
        "    unique_entity_clusters[\"Q248\"].append(\"intel corp\")\n",
        "else:\n",
        "  print(\"Unique cluster does not exist.\")\n",
        "  unique_entity_clusters[\"Q248\"] = [\"intel corp\"]\n",
        "  print(\"Added unique cluster and entity.\")\n",
        "\n",
        "print(\"The final unique entity clusters are:\")\n",
        "print(unique_entity_clusters)"
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Cluster exists\n",
            "The final unique entity clusters are:\n",
            "{'Q248': ['intel llc', 'intel corp'], 'NIL': ['london, great'], 'Q714491': ['marks and spencer ltd'], 'Q48': ['east asia']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtF5RIeCxb64"
      },
      "source": [
        "Replicating the above process with the final string. This is the code that would be implmented in production. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glYab-DFxObI",
        "outputId": "97c1583d-116a-4b07-9b8c-c93910d92780"
      },
      "source": [
        "unique_entity_clusters = {} \n",
        "for text in string_list:\n",
        "    print(\"String \", text)\n",
        "    #print(f\"Gold annotation: {true_annot}\")\n",
        "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
        "        if ent.kb_id_ in unique_entity_clusters:\n",
        "            \"Unique Cluster exists\"\n",
        "            if ent.text in unique_entity_clusters[ent.kb_id_]:\n",
        "              print(\"Entity already exists\")\n",
        "              print(\"\")\n",
        "            else:\n",
        "              unique_entity_clusters[ent.kb_id_].append(ent.text)\n",
        "              print(\"\")\n",
        "        else:\n",
        "            print(\"Unique cluster does not exist.\")\n",
        "            unique_entity_clusters[ent.kb_id_] = [ent.text]\n",
        "            print(\"Added unique cluster and entity.\")\n",
        "            print(\"\")\n",
        "\n",
        "print(\"The final unique entity clusters are:\")\n",
        "print(unique_entity_clusters)"
      ],
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "String  marks and spencer ltd\n",
            "Prediction: marks and spencer ltd, ORG, Q714491\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  london\n",
            "Prediction: london, LOC, Q84\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  icnao02312\n",
            "Prediction: icnao02312, ORG, NIL\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  london, great britain\n",
            "Prediction: london, great, LOC, NIL\n",
            "\n",
            "String  toys\n",
            "Prediction: toys, LOC, P6\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  intel llc\n",
            "Prediction: intel llc, LOC, Q248\n",
            "Unique cluster does not exist.\n",
            "Added unique cluster and entity.\n",
            "\n",
            "String  m&s corporation limited\n",
            "String  london, england\n",
            "Prediction: london, england, LOC, Q84\n",
            "\n",
            "The final unique entity clusters are:\n",
            "{'Q714491': ['marks and spencer ltd'], 'Q84': ['london', 'london, england'], 'NIL': ['icnao02312', 'london, great'], 'P6': ['toys'], 'Q248': ['intel llc']}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}